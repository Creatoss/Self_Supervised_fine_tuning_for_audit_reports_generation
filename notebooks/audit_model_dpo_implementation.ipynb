{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO Fine-Tuning for Audit Report Generation\n",
    "\n",
    "This notebook implements **Direct Preference Optimization (DPO)** to align your fine-tuned Mistral model to:\n",
    "1. **Stop hallucinating** - Only use facts from provided data\n",
    "2. **Follow audit report structure** - Generate professional, structured reports\n",
    "3. **Avoid repetition** - Produce coherent, non-looping text\n",
    "\n",
    "## What is DPO?\n",
    "DPO teaches the model to prefer \"good\" responses over \"bad\" ones by training on pairs:\n",
    "- ‚úÖ **Chosen**: Accurate, grounded, professional audit text\n",
    "- ‚ùå **Rejected**: Hallucinated, repetitive, or poorly structured text\n",
    "\n",
    "## Workflow\n",
    "1. Load your self-supervised fine-tuned model\n",
    "2. Create preference dataset (good vs bad examples)\n",
    "3. Train with DPO\n",
    "4. Evaluate improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers peft datasets bitsandbytes accelerate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x790cb9607630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úÖ Google Drive mounted!\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive mounted!\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Not in Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Fine-Tuned Model\n",
    "We load the model you already trained with self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2873895474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading base model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mBASE_MODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4061\u001b[0m             \u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m         )\n\u001b[0;32m-> 4063\u001b[0;31m         \u001b[0mloading_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4064\u001b[0m         \u001b[0mloading_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_model_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloading_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4065\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(model, state_dict, checkpoint_files, load_config)\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_hqq_or_quark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4141\u001b[0m             \u001b[0mexpanded_device_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4142\u001b[0;31m             \u001b[0mcaching_allocator_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_device_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_quantizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4144\u001b[0m         \u001b[0merror_msgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mcaching_allocator_warmup\u001b[0;34m(model, expanded_device_map, hf_quantizer)\u001b[0m\n\u001b[1;32m   4682\u001b[0m             \u001b[0mtorch_accelerator_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4683\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch_accelerator_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4684\u001b[0;31m             \u001b[0mdevice_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_accelerator_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4685\u001b[0m             \u001b[0;31m# Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4686\u001b[0m             \u001b[0;31m# than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mmem_get_info\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;31m# optional=True allows `device = torch.device('cuda')` for which device.index is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaMemGetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Clear memory\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "BASE_MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "FINETUNED_MODEL_PATH = \"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-mistral-7b-qlora\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    max_memory={0: \"14GB\", \"cpu\": \"30GB\"}\n",
    ")\n",
    "\n",
    "print(\"Loading fine-tuned adapters...\")\n",
    "model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "\n",
    "# Merge adapters into base model for DPO training\n",
    "print(\"Merging adapters...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Prepare for new LoRA training (DPO)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Model loaded and ready for DPO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DPO Preference Dataset\n",
    "\n",
    "**Format**: Each example has:\n",
    "- `prompt`: The instruction/query\n",
    "- `chosen`: Good response (grounded in data, professional)\n",
    "- `rejected`: Bad response (hallucinated, repetitive, or off-topic)\n",
    "\n",
    "**TODO**: Replace the examples below with your own audit-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import torch\n",
    "# --- Configuration ---\n",
    "GEMINI_API_KEY = \"AIzaSyA9kPKdLbfK3PENP6bjQjFtajWtl0hXpXY\" \n",
    "NUM_SAMPLES = 10 \n",
    "# Initialize Gemini Client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "# Use the new 2.5 Flash model for high-quality corrections\n",
    "JUDGE_MODEL_ID = \"gemini-2.5-flash\"\n",
    "# List of questions\n",
    "questions = [\n",
    "    \"Draft the Revenue Recognition section for a company with ¬£109.1 million turnover.\",\n",
    "    \"Describe the independence threats related to non-audit fees.\",\n",
    "    \"What were the key weaknesses identified in the audit inspections?\",\n",
    "    \"Explain the concept of professional skepticism in auditing.\",\n",
    "    \"How should an auditor determine materiality?\",\n",
    "    \"What are the auditor's responsibilities regarding fraud?\",\n",
    "    \"Summarize the findings on Going Concern assessments.\",\n",
    "    \"What are the requirements for partner rotation?\",\n",
    "    \"Draft an opinion on the financial statements.\",\n",
    "    \"Explain the audit risk model components.\"\n",
    "]\n",
    "def get_gemini_correction(prompt, bad_response):\n",
    "    \"\"\"\n",
    "    Asks Gemini to rewrite the bad response into a perfect audit response.\n",
    "    \"\"\"\n",
    "    correction_prompt = f\"\"\"\n",
    "    You are an expert Audit Partner at a Big 4 firm. I will give you a Question and a Draft Answer.\n",
    "    The Draft Answer might be repetitive, hallucinated, or unprofessionally written.\n",
    "    \n",
    "    **Task**: rewrite the answer to be:\n",
    "    1. Factually accurate (generalize if specific numbers in draft are suspicious/hallucinated).\n",
    "    2. Professional, concise, and structured (like a real audit report).\n",
    "    3. Free of repetition or looping text.\n",
    "    \n",
    "    **Question**: {prompt}\n",
    "    **Draft Answer (Rejected)**: {bad_response}\n",
    "    \n",
    "    **Output**: Just the corrected text. No preamble.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=JUDGE_MODEL_ID,\n",
    "            contents=correction_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.2,\n",
    "                max_output_tokens=1024\n",
    "            )\n",
    "        )\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return None\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    # 1. Generate 'Rejected' response from your model\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        # Generate with parameters that might induce current bad behavior (to catch it)\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=200, \n",
    "            temperature=0.7, \n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.0 # Low penalty to catch repetition if it exists\n",
    "        )\n",
    "    \n",
    "    # Decode and strip the prompt to get just the response\n",
    "    rejected_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the start if it repeats\n",
    "    if rejected_response.startswith(q):\n",
    "        rejected_response = rejected_response[len(q):].strip()\n",
    "        \n",
    "    print(f\"\\nüìù Question: {q}\")\n",
    "    print(f\"‚ùå Rejected (Model): {rejected_response[:100]}...\")\n",
    "    \n",
    "    # 2. Get 'Chosen' response from Gemini\n",
    "    chosen_response = get_gemini_correction(q, rejected_response)\n",
    "    \n",
    "    if chosen_response:\n",
    "        print(f\"‚úÖ Chosen (Gemini): {chosen_response[:100]}...\")\n",
    "        \n",
    "        # 3. Add to dataset\n",
    "        dpo_data.append({\n",
    "            \"prompt\": q,\n",
    "            \"chosen\": chosen_response,\n",
    "            \"rejected\": rejected_response\n",
    "        })\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping due to API error\")\n",
    "        \n",
    "    # Sleep briefly to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Convert to Dataset for training\n",
    "dpo_dataset = Dataset.from_list(dpo_data)\n",
    "print(f\"\\n‚ú® Generated {len(dpo_dataset)} DPO pairs successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure DPO Training\n",
    "We add new LoRA adapters on top of the merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New LoRA config for DPO\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# DPO Training Arguments\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./audit-mistral-dpo\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,  # Lower LR for DPO\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    beta=0.1,  # DPO temperature (how strongly to prefer chosen over rejected)\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DPO configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DPO Trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # Use implicit reference model (saves memory)\n",
    "    args=training_args,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting DPO training...\")\n",
    "dpo_trainer.train()\n",
    "print(\"‚úÖ DPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the DPO-Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DPO adapters\n",
    "output_path = \"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-mistral-7b-dpo\"\n",
    "dpo_trainer.save_model(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"‚úÖ DPO model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the DPO-Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DPO model for inference\n",
    "print(\"Loading DPO model for testing...\")\n",
    "test_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "test_model = PeftModel.from_pretrained(test_model, output_path)\n",
    "test_model.eval()\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Draft the Revenue Recognition section for a company with ¬£109.1 million turnover.\",\n",
    "    \"Describe independence threats when non-audit fees exceed audit fees by 400%.\",\n",
    "    \"What were the key weaknesses in going concern assessments?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING DPO-ALIGNED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüìã PROMPT: {prompt}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from response\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"ü§ñ RESPONSE:\\n{response}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare: Before vs After DPO\n",
    "Load both models and compare their outputs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BEFORE DPO model (original fine-tuned)...\")\n",
    "before_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "before_model = PeftModel.from_pretrained(before_model, FINETUNED_MODEL_PATH)\n",
    "before_model.eval()\n",
    "\n",
    "test_prompt = \"Describe independence threats when non-audit fees exceed audit fees by 400%.\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: BEFORE vs AFTER DPO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìã PROMPT: {test_prompt}\\n\")\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Before DPO\n",
    "print(\"-\"*70)\n",
    "print(\"‚ùå BEFORE DPO (Original Fine-Tuned Model):\")\n",
    "print(\"-\"*70)\n",
    "with torch.no_grad():\n",
    "    before_output = before_model.generate(**inputs, max_new_tokens=150, temperature=0.3, repetition_penalty=1.1)\n",
    "before_response = tokenizer.decode(before_output[0], skip_special_tokens=True)[len(test_prompt):].strip()\n",
    "print(before_response)\n",
    "\n",
    "# After DPO\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"‚úÖ AFTER DPO (Aligned Model):\")\n",
    "print(\"-\"*70)\n",
    "with torch.no_grad():\n",
    "    after_output = test_model.generate(**inputs, max_new_tokens=150, temperature=0.3, repetition_penalty=1.2)\n",
    "after_response = tokenizer.decode(after_output[0], skip_special_tokens=True)[len(test_prompt):].strip()\n",
    "print(after_response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
