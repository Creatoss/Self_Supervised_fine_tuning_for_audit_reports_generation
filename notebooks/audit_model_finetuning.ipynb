{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Fine-Tuning of Mistral-7B on Audit Reports\n",
    "\n",
    "This notebook demonstrates how to adapt a Large Language Model (Mistral-7B) to the domain of professional audit reports using self-supervised fine-tuning (continued pretraining). \n",
    "\n",
    "**Objective**: Enhance the model's domain fluency, vocabulary, and stylistic consistency for audit documentation.\n",
    "**Method**: Causal Language Modeling (Next-Token Prediction) on raw text extracted from PDF reports.\n",
    "**Hardware**: Optimized for a T4 GPU (Google Colab free tier compatible) using QLoRA (4-bit quantization + LoRA).\n",
    "\n",
    "## 1. Setup and Installation\n",
    "We need to install the necessary libraries for PDF extraction, efficient model loading, and training.\n",
    "\n",
    "**IMPORTANT**: After running the installation cell below, you MUST restart the runtime/session (Runtime > Restart session) for the updates to take effect, then run the cells starting from the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cuda-python 12.9.5 requires cuda-bindings~=12.9.5, but you have cuda-bindings 12.9.4 which is incompatible.\n",
      "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstallation complete. Please RESTART the runtime (Runtime > Restart session) to apply changes, then run the next cells.\n"
     ]
    }
   ],
   "source": [
    "# Install all key dependencies including PyTorch components to ensure version compatibility\n",
    "!pip install -q -U torch torchvision torchaudio transformers peft datasets bitsandbytes trl pdfplumber accelerate\n",
    "\n",
    "print(\"Installation complete. Please RESTART the runtime (Runtime > Restart session) to apply changes, then run the next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7869967edeb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pdfplumber\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import re\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Mounted Google Drive. DATA_DIR set to: /content/drive/MyDrive/Data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "    except:\n",
    "        pass\n",
    "    # Update DATA_DIR to point to mounted Drive\n",
    "    # Make sure you have uploaded the Data folder to your Google Drive root\n",
    "    DATA_DIR = Path('/content/drive/MyDrive/Data')\n",
    "    print(f\"Mounted Google Drive. DATA_DIR set to: {DATA_DIR}\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"./Data\")\n",
    "    print(f\"Not running in Colab. Using local Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We will extract text from the PDF audit reports located in the `Data` directory. \n",
    "\n",
    "**Cleaning Steps**:\n",
    "- Extract text using `pdfplumber`.\n",
    "- Remove potential headers and footers (heuristic: very short lines at top/bottom of pages).\n",
    "- Normalize whitespace.\n",
    "- Anonymize sensitive patterns (placeholder implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PDFs in: /content/drive/MyDrive/Data\n",
      "Processing /content/drive/MyDrive/Data/Annual_Review_of_Audit_Quality_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/BDO_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Deloitte_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Ernst__Young_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Forvis_Mazars_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/KPMG_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/PricewaterhouseCoopers_LLP_Audit_Quality_Inspection_and_Supervision_2025.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Annual_Review_of_Audit_Quality_2024_7yhxTsi.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Tier_1_Firms__Overview_2023.pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_Audit_Quality_Inspection_and_Supervision_Public_Report_2022_-_Tier_1_Firms_Overview.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Individual_Rights_Data_Privacy_Policy.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Deloitte_LLP_Audit_Quality_Inspection_and_Supervision_Report_2024.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Deloitte_LLP_Audit_Quality_Inspection_and_Supervision_Report_2023.pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_Deloitte_LLP_Public_Report.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Deloitte_-_FRC_Audit_Quality_Inspection_and_Supervision_report_-_23_July_2021.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Deloitte_Audit_Quality_Inspection_Report_Jul_2020.pdf...\n",
      "Processing /content/drive/MyDrive/Data/19.pdf...\n",
      "Processing /content/drive/MyDrive/Data/18.pdf...\n",
      "Processing /content/drive/MyDrive/Data/17_-_Deloitte_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Audit_Quality_Inspection_Report_May_2016_Deloitte_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/15_Deloitte_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/PricewaterhouseCoopers_LLP_Audit_Quality_Inspection_and_Supervision_Report_2024.pdf...\n",
      "Processing /content/drive/MyDrive/Data/PricewaterhouseCoopers_LLP_Audit_Quality_Inspection_and_Supervision_Report_2023.pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_PricewaterhouseCoopers_LLP_Public_Report.pdf...\n",
      "Processing /content/drive/MyDrive/Data/PwC_-_FRC_Audit_Quality_Inspection_and_Supervision_report_-_23_July_2021.pdf...\n",
      "Processing /content/drive/MyDrive/Data/PwC_Audit_Quality_Inspection_Report_Jul_2020.pdf...\n",
      "Processing /content/drive/MyDrive/Data/19 (1).pdf...\n",
      "Processing /content/drive/MyDrive/Data/18 (1).pdf...\n",
      "Processing /content/drive/MyDrive/Data/17_-_PwC_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Audit_Quality_Inspection_Report_May_2016_PricewaterhouseCoopers_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/15_PricewaterhouseCoopers_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_Audit_Quality_Inspection_and_Supervision_Public_Report_2022_-_Tier_1_Firms_Overview (2).pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_Audit_Quality_Inspection_and_Supervision_Public_Report_2022_-_Tier_1_Firms_Overview (1).pdf...\n",
      "Processing /content/drive/MyDrive/Data/KPMG_LLP_Audit_Quality_Inspection_and_Supervision_Report_2024.pdf...\n",
      "Processing /content/drive/MyDrive/Data/KPMG_LLP_Audit_Quality_Inspection_and_Supervision_Report_2023.pdf...\n",
      "Processing /content/drive/MyDrive/Data/FRC_KPMG_LLP_Public_Report.pdf...\n",
      "Processing /content/drive/MyDrive/Data/KPMG_-_FRC_Audit_Quality_Inspection_and_Supervision_report_-_23_July_2021.pdf...\n",
      "Processing /content/drive/MyDrive/Data/KPMG_Audit_Quality_Inspection_Report_Jul_2020.pdf...\n",
      "Processing /content/drive/MyDrive/Data/19 (2).pdf...\n",
      "Processing /content/drive/MyDrive/Data/18 (2).pdf...\n",
      "Processing /content/drive/MyDrive/Data/17_-_KPMG_LLP.pdf...\n",
      "Processing /content/drive/MyDrive/Data/Audit_Quality_Inspection_Report_May_2016_KPMG_LLP_and_KPMG_Audit_Plc.pdf...\n",
      "Processing /content/drive/MyDrive/Data/15_KPMG_LLP_and_KPMG_Audit_Plc.pdf...\n",
      "\n",
      "Successfully loaded 43 documents.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_content = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            lines = text.split('\\n')\n",
    "            \n",
    "            # Basic Heuristic: Remove first and last lines if they likely resemble headers/footers (e.g., page numbers or short titles)\n",
    "            # Adjust this logic based on your specific PDF layout\n",
    "            if len(lines) > 2:\n",
    "                # Remove header if short (arbitrary length < 50 chars as a heuristic)\n",
    "                if len(lines[0]) < 50:\n",
    "                    lines = lines[1:]\n",
    "                # Remove footer if short and looks like page number\n",
    "                if len(lines) > 0 and len(lines[-1]) < 20:\n",
    "                    lines = lines[:-1]\n",
    "            \n",
    "            page_text = \"\\n\".join(lines)\n",
    "            text_content.append(page_text)\n",
    "    \n",
    "    full_text = \"\\n\\n\".join(text_content)\n",
    "    return full_text\n",
    "\n",
    "def clean_data(text):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Placeholder for anonymization (e.g., replace emails, phone numbers)\n",
    "    # This regex is a simple example and should be expanded for real production use\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Main Data Loading Loop\n",
    "try:\n",
    "    data_dir = DATA_DIR\n",
    "except NameError:\n",
    "    data_dir = \"./Data\"\n",
    "    \n",
    "print(f\"Searching for PDFs in: {data_dir}\")\n",
    "pdf_files = glob.glob(str(data_dir / \"*.pdf\"))\n",
    "\n",
    "raw_texts = []\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing {pdf_file}...\")\n",
    "    try:\n",
    "        raw_text = extract_text_from_pdf(pdf_file)\n",
    "        cleaned_text = clean_data(raw_text)\n",
    "        if len(cleaned_text) > 500: # Only keep documents with substantial content\n",
    "            raw_texts.append(cleaned_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(raw_texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Tokenization and Chunking\n",
    "\n",
    "We need to process the text into chunks suitable for the model's context window. \n",
    "- **Context Window**: 1024 tokens (Reduced from 2048 to save VRAM).\n",
    "- **Overlap**: No overlap in packing strategy.\n",
    "- **Format**: Prepare as a Hugging Face Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 38\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed0e719a8c242e8bf3e20caf8a2981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3919ee3f2f4d6d8d1e411a31338a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236910cc9be4bcaa38053a2b6b70287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b60c4b349f44db91182f3c2a086cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb6fc83fb7b4a65bb3d50bb8ef15365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1564f996b2a94d7ca18e0f8cf6e9997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking and Tokenizing:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65ca9efdbdd4547a158915ca1aa6375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking and Tokenizing:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train chunks: 488\n",
      "Test chunks: 58\n"
     ]
    }
   ],
   "source": [
    "# Create HF Dataset\n",
    "dataset = Dataset.from_dict({\"text\": raw_texts})\n",
    "\n",
    "# Split into train and validation\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "print(dataset)\n",
    "\n",
    "# Load Tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Mistral has no pad token by default\n",
    "\n",
    "def chunk_and_tokenize(examples):\n",
    "    # Flatten texts into a single long string of tokens\n",
    "    chunk_size = 1024 # Reduced from 2048 to save VRAM\n",
    "    \n",
    "    # Basic tokenization without padding/truncated\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=False, return_attention_mask=False)[\"input_ids\"]\n",
    "    \n",
    "    # Flatten list of lists into one big list of tokens\n",
    "    concatenated_tokens = [tok for doc in tokens for tok in doc]\n",
    "    \n",
    "    # Calculate total length divisible by chunk_size\n",
    "    # We drop the small remainder at the very end of the entire dataset\n",
    "    total_length = len(concatenated_tokens)\n",
    "    if total_length >= chunk_size:\n",
    "        total_length = (total_length // chunk_size) * chunk_size\n",
    "    else:\n",
    "        # Handle highly unlikely case where entire dataset < chunk_size tokens\n",
    "        # Pad to chunk_size\n",
    "        concatenated_tokens += [tokenizer.eos_token_id] * (chunk_size - total_length)\n",
    "        total_length = chunk_size\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        \"input_ids\": [concatenated_tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)],\n",
    "        \"labels\": [concatenated_tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    chunk_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Chunking and Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Train chunks: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Test chunks: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading with QLoRA\n",
    "\n",
    "We load Mistral-7B in 4-bit quantization to fit on a T4 GPU.\n",
    "Then we attach LoRA adapters for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70993073f9384b18afc60e5a259122a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906324693b6f4606820427da5bf116fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac9cf4e7099485ab45fefe7db6fbf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1383cea149894eee8cbd49284251b66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5ad197392e49f997bcd608c6e52d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n"
     ]
    }
   ],
   "source": [
    "# 4-bit Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, # or bfloat16 if supported by hardware\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We use the basic `Trainer` with `DataCollatorForLanguageModeling`. \n",
    "The objective is purely self-supervised next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 3:05:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.041756</td>\n",
       "      <td>1.821471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.909383</td>\n",
       "      <td>1.722224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.843072</td>\n",
       "      <td>1.622767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.711572</td>\n",
       "      <td>1.553089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.670609</td>\n",
       "      <td>1.482170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.618916</td>\n",
       "      <td>1.416355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.363736</td>\n",
       "      <td>1.375976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.248240</td>\n",
       "      <td>1.336936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.290128</td>\n",
       "      <td>1.306720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.201266</td>\n",
       "      <td>1.269688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.261886</td>\n",
       "      <td>1.239418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.150999</td>\n",
       "      <td>1.217538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.989004</td>\n",
       "      <td>1.229427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.942599</td>\n",
       "      <td>1.206252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.915806</td>\n",
       "      <td>1.183133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.886430</td>\n",
       "      <td>1.172996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.912087</td>\n",
       "      <td>1.163020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.957014</td>\n",
       "      <td>1.164171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=183, training_loss=1.3194929198489163, metrics={'train_runtime': 11170.02, 'train_samples_per_second': 0.131, 'train_steps_per_second': 0.016, 'total_flos': 6.433634912934298e+16, 'train_loss': 1.3194929198489163, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./audit-mistral-finetuned\",\n",
    "    per_device_train_batch_size=1, # Reduced to 1 to fit T4 VRAM\n",
    "    gradient_accumulation_steps=8, # Increased to 8 to maintain effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3, # Increased to 3 epochs\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\", # Evaluate more frequently\n",
    "    eval_steps=10,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\", # Memory efficient optimizer\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We calculate Perplexity as a quantitative metric of how well the model predicts the domain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.20\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference\n",
    "\n",
    "Test the model's generation capabilities on an audit-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Audit Matter: Revenue Recognition\n",
      "The Group reported revenue of $4,520 million for the year ended 31 December 2024 (2023: $4,105 million).\n",
      "The risk is that revenue may be overstated due to the pressure management may feel to achieve performance targets. We focused on the risk of cut-off and the valuation of unbilled revenue at year-end.\n",
      "Our audit procedures included:\n",
      "•  Agreement of the cut-off policy with management.\n",
      "•  A cut-off compliance testing programme.\n",
      "•  Evaluating the completeness of revenue cut-off procedures for the largest revenue stream.\n",
      "•  A valuation of unbilled revenue at year-end for a sample of material contracts, including consideration of the impact of post-balance sheet events.\n",
      "Further details of our work on revenue can be found in Section 2 of this report.\n",
      "Further information on the firm’s internal procedures which support the audit work on revenue can be found in the firm’s Quality Control procedures in Appendix A. 10 KPMG LLP – Audit Quality Inspection (July\n"
     ]
    }
   ],
   "source": [
    "# Save the model (adapters only)\n",
    "#trainer.save_model(\"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-mistral-7b-qlora\")\n",
    "\n",
    "# Inference Prompt\n",
    "prompt = \"how audit is done and what meatrics y use\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-mistral-7b-qlora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity | General Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Advanced Analysis ---\n",
      "\n",
      "Calculating Cosine Similarity (Base vs Fine-Tuned)...\n",
      "  Sentence: 'The audit committee is responsible for o...' | Similarity: 0.7966\n",
      "  Sentence: 'Material misstatements can arise from fr...' | Similarity: 0.7899\n",
      "  Sentence: 'We conducted our audit in accordance wit...' | Similarity: 0.7636\n",
      "Average Domain Representation Consistency: 0.7833\n",
      "\n",
      "Calculating General Knowledge Retention (Perplexity)...\n",
      "Perplexity on General Topics: 5.72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7833432952562968), 5.718234062194824)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- FIX: Ensure Adapters are Active ---\n",
    "# If the model is just a base model, we attach the trained adapters we just formatted\n",
    "if not isinstance(model, PeftModel):\n",
    "    # If we just finished training, the model might still be wrapped in the trainer\n",
    "    # We can try to use the model directly if it already has peft config\n",
    "    if hasattr(model, \"peft_config\"):\n",
    "         # It's already a PeftModel, just maybe not typed correctly or in a weird state\n",
    "         pass\n",
    "    else:\n",
    "        # Worst case: Load adapters from the checkpoint we just saved (Safety net)\n",
    "        # This assumes you ran trainer.save_model() in the previous cell\n",
    "        adapter_path = \"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-mistral-7b-qlora\"\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, adapter_path)\n",
    "            print(\"Loaded adapters from disk.\")\n",
    "        except:\n",
    "            print(\"Could not load adapters from disk. Using current model state.\")\n",
    "\n",
    "# 1. Define Helper to get Embeddings\n",
    "def get_sentence_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    return last_hidden_state.mean(dim=1) \n",
    "\n",
    "# 2. Define Metrics Calculation\n",
    "def analyze_model_performance(model, tokenizer):\n",
    "    print(\"--- Starting Advanced Analysis ---\\n\")\n",
    "    \n",
    "    audit_sentences = [\n",
    "        \"The audit committee is responsible for overseeing the financial reporting process.\",\n",
    "        \"Material misstatements can arise from fraud or error in the financial statements.\",\n",
    "        \"We conducted our audit in accordance with International Standards on Auditing (UK).\"\n",
    "    ]\n",
    "    \n",
    "    general_sentences = [\n",
    "        \"The cat sat on the mat and looked out the window at the rain.\",\n",
    "        \"The capital of France is Paris, which is known for its culture and history.\",\n",
    "        \"Photosynthesis is the process by which plants use sunlight to create energy.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Calculating Cosine Similarity (Base vs Fine-Tuned)...\")\n",
    "    similarities = []\n",
    "    \n",
    "    for text in audit_sentences:\n",
    "        # 1. Get Fine-Tuned Embedding\n",
    "        # Instead of model.enable_adapters(), we use the PEFT context manager if available, \n",
    "        # or just assume the current state is the fine-tuned state.\n",
    "        try:\n",
    "             model.enable_adapters()\n",
    "        except:\n",
    "             pass # If it fails, we assume adapters are already active or merged\n",
    "             \n",
    "        ft_emb = get_sentence_embedding(model, tokenizer, text)\n",
    "        \n",
    "        # 2. Get Base Model Embedding\n",
    "        # We try to disable adapters. If we can't, we skip this metric to avoid crashing.\n",
    "        try:\n",
    "            with model.disable_adapter():\n",
    "                base_emb = get_sentence_embedding(model, tokenizer, text)\n",
    "            \n",
    "            sim = cosine_similarity(ft_emb, base_emb).item()\n",
    "            similarities.append(sim)\n",
    "            print(f\"  Sentence: '{text[:40]}...' | Similarity: {sim:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not separate Base/Adapter embeddings: {e}\")\n",
    "            similarities.append(1.0) # Fallback\n",
    "\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    print(f\"Average Domain Representation Consistency: {avg_similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nCalculating General Knowledge Retention (Perplexity)...\")\n",
    "    encodings = tokenizer(\"\\n\\n\".join(general_sentences), return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings.input_ids, labels=encodings.input_ids)\n",
    "        general_loss = outputs.loss\n",
    "    \n",
    "    general_perplexity = torch.exp(general_loss).item()\n",
    "    print(f\"Perplexity on General Topics: {general_perplexity:.2f}\")\n",
    "\n",
    "    return avg_similarity, general_perplexity\n",
    "\n",
    "analyze_model_performance(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
