{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Fine-Tuning of Llama-2-7B on Audit Reports\n",
    "\n",
    "This notebook adapts **Llama-2-7B** to the domain of professional audit reports using self-supervised fine-tuning (continued pretraining).\n",
    "\n",
    "**Objective**: Enhance Llama-2's domain fluency for audit documentation.\n",
    "**Model**: `NousResearch/Llama-2-7b-hf` (Non-gated Llama 2 base model).\n",
    "**Method**: QLoRA (4-bit quantization + LoRA) on T4 GPU.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "**IMPORTANT**: After installation, restart the runtime (Runtime > Restart session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U torch torchvision torchaudio transformers peft datasets bitsandbytes trl pdfplumber accelerate\n",
    "\n",
    "print(\"Installation complete. Please RESTART the runtime now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pdfplumber\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "import re\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "    except:\n",
    "        pass\n",
    "    DATA_DIR = Path('/content/drive/MyDrive/Data')\n",
    "    print(f\"Mounted Google Drive. DATA_DIR set to: {DATA_DIR}\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"./Data\")\n",
    "    print(f\"Using local Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "Extract text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_content = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if not text: continue\n",
    "            lines = text.split('\\n')\n",
    "            # Heuristic cleaning\n",
    "            if len(lines) > 2:\n",
    "                if len(lines[0]) < 50: lines = lines[1:]\n",
    "                if len(lines) > 0 and len(lines[-1]) < 20: lines = lines[:-1]\n",
    "            text_content.append(\"\\n\".join(lines))\n",
    "    return \"\\n\\n\".join(text_content)\n",
    "\n",
    "def clean_data(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    pdf_files = glob.glob(str(DATA_DIR / \"*.pdf\"))\n",
    "except:\n",
    "    pdf_files = glob.glob(\"./Data/*.pdf\")\n",
    "\n",
    "raw_texts = []\n",
    "print(f\"Found {len(pdf_files)} PDFs.\")\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        raw_text = extract_text_from_pdf(pdf_file)\n",
    "        cleaned_text = clean_data(raw_text)\n",
    "        if len(cleaned_text) > 500:\n",
    "            raw_texts.append(cleaned_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(raw_texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Tokenizer (Llama 2)\n",
    "We use `NousResearch/Llama-2-7b-hf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = Dataset.from_dict(\"text\": raw_texts)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Load Llama 2 Tokenizer\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def chunk_and_tokenize(examples):\n",
    "    chunk_size = 1024\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=False, return_attention_mask=False)[\"input_ids\"]\n",
    "    concatenated_tokens = [tok for doc in tokens for tok in doc]\n",
    "    total_length = len(concatenated_tokens)\n",
    "    if total_length >= chunk_size:\n",
    "        total_length = (total_length // chunk_size) * chunk_size\n",
    "    else:\n",
    "        concatenated_tokens += [tokenizer.eos_token_id] * (chunk_size - total_length)\n",
    "        total_length = chunk_size\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": [concatenated_tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)],\n",
    "        \"labels\": [concatenated_tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    chunk_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA Setup for Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config for Llama 2\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./audit-llama2-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Advanced Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Perplexity\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Cosine Similarity & Forgetting\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "if not isinstance(model, PeftModel):\n",
    "    try:\n",
    "        # Try to reload if needed\n",
    "        pass \n",
    "    except: pass\n",
    "\n",
    "def get_sentence_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "def analyze_model_performance(model, tokenizer):\n",
    "    print(\"--- Advanced Analysis ---\")\n",
    "    audit_sents = [\"The audit committee oversees financial reporting.\", \"Material misstatements can arise from fraud.\"]\n",
    "    general_sents = [\"The cat sat on the mat.\", \"Paris is the capital of France.\"]\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    sims = []\n",
    "    for text in audit_sents:\n",
    "        try: model.enable_adapters()\n",
    "        except: pass\n",
    "        ft_emb = get_sentence_embedding(model, tokenizer, text)\n",
    "        try:\n",
    "            with model.disable_adapter():\n",
    "                base_emb = get_sentence_embedding(model, tokenizer, text)\n",
    "            sims.append(cosine_similarity(ft_emb, base_emb).item())\n",
    "        except: sims.append(1.0)\n",
    "    \n",
    "    print(f\"Avg Domain Consistency: {np.mean(sims):.4f}\")\n",
    "    \n",
    "    # General Perplexity\n",
    "    print(\"Checking General Knowledge...\")\n",
    "    try: model.enable_adapters()\n",
    "    except: pass\n",
    "    enc = tokenizer(\"\\n\".join(general_sents), return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        loss = model(enc.input_ids, labels=enc.input_ids).loss\n",
    "    print(f\"General Perplexity: {torch.exp(loss).item():.2f}\")\n",
    "\n",
    "analyze_model_performance(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "save_path = \"/content/drive/MyDrive/Self_Supervised_finetuning_Model/audit-llama2-7b-qlora\"\n",
    "trainer.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
